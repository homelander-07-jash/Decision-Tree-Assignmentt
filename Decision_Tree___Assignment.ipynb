{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree | Assignment\n",
        "\n",
        "1. What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "- A Decision Tree is a supervised machine learning algorithm used for classification and regression problems. It works by splitting the dataset into smaller subsets based on feature values.\n",
        "\n",
        "In classification, the tree starts with a root node that represents the entire dataset. It then selects the best feature to split the data based on a criterion like Gini Impurity or Entropy. Each split creates branches, and this process continues until a stopping condition is met.\n",
        "The final nodes are called leaf nodes, and each leaf node represents a class label.\n",
        "\n",
        "Decision Trees are easy to understand and visualize, making them very useful for explaining model decisions.\n",
        "\n",
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "- Gini Impurity and Entropy are measures used to check how “pure” or “impure” a node is.\n",
        "\n",
        "- Gini Impurity measures the probability that a randomly chosen data point would be incorrectly classified.\n",
        "\n",
        "- Lower Gini value means purer node.\n",
        "\n",
        "- Entropy measures the level of disorder or uncertainty in the data.\n",
        "\n",
        "- Lower entropy means less uncertainty.\n",
        "\n",
        "When building a Decision Tree, the algorithm chooses splits that reduce impurity the most.\n",
        "Better splits result in nodes where most samples belong to one class, improving classification accuracy.\n",
        "\n",
        "\n",
        "\n",
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "- Pre-Pruning stops the tree from growing too deep by setting limits like maximum depth or minimum samples per split.\n",
        "\n",
        "- Advantage: Reduces overfitting and makes the model faster to train.\n",
        "\n",
        "- Post-Pruning allows the tree to grow fully and then removes unnecessary branches.\n",
        "\n",
        "- Advantage: Often gives better performance because pruning decisions are made after seeing the full tree.\n",
        "\n",
        "4.  What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "- Information Gain measures how much uncertainty is reduced after splitting the data on a particular feature.\n",
        "It is calculated as the difference between the entropy before the split and the weighted entropy after the split.\n",
        "\n",
        "The feature with the highest Information Gain is chosen for splitting because it gives the most useful information and leads to better classification.\n",
        "\n",
        "5. What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "- Applications:\n",
        "\n",
        "Medical diagnosis\n",
        "\n",
        "Credit risk assessment\n",
        "\n",
        "Spam email detection\n",
        "\n",
        "Customer churn prediction\n",
        "\n",
        "Fraud detection\n",
        "\n",
        "- Advantages:\n",
        "\n",
        "Easy to understand and interpret\n",
        "\n",
        "Works with both numerical and categorical data\n",
        "\n",
        "Requires little data preprocessing\n",
        "\n",
        "- Limitations:\n",
        "\n",
        "Can easily overfit the data\n",
        "\n",
        "Sensitive to small changes in data\n",
        "\n",
        "Not as accurate as ensemble methods like Random Forests"
      ],
      "metadata": {
        "id": "P9phleY8gAV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Write a Python program to:\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model = DecisionTreeClassifier(criterion=\"gini\")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Feature importances\n",
        "print(\"Feature Importances:\", model.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gnUASMyiZxX",
        "outputId": "25569795-3bdb-4092-900a-213a9f8d63f5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.01667014 0.         0.90614339 0.07718647]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 Write a Python program to:\n",
        "# Load the Iris Dataset\n",
        "# Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "# fully-grown tree.\n",
        "\n",
        "# Fully grown tree\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "full_acc = accuracy_score(y_test, full_tree.predict(X_test))\n",
        "\n",
        "# Pruned tree\n",
        "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "pruned_tree.fit(X_train, y_train)\n",
        "pruned_acc = accuracy_score(y_test, pruned_tree.predict(X_test))\n",
        "\n",
        "print(\"Fully-grown tree accuracy:\", full_acc)\n",
        "print(\"Max depth = 3 accuracy:\", pruned_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDT7kqztjcL3",
        "outputId": "30da3d99-ca5b-4ac9-a00a-fd38a009f2ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully-grown tree accuracy: 1.0\n",
            "Max depth = 3 accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8 Write a Python program to:\n",
        "# Load the Boston Housing Dataset\n",
        "# Train a Decision Tree Regressor\n",
        "# Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train regressor\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "# MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "# Feature importances\n",
        "print(\"Feature Importances:\", reg.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpZ_fxkvkGLh",
        "outputId": "1a62580c-7535-4beb-bc49-224b1723d57b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.495235205629094\n",
            "Feature Importances: [0.52850909 0.05188354 0.05297497 0.02866046 0.03051568 0.13083768\n",
            " 0.09371656 0.08290203]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9 Write a Python program to:\n",
        "# Load the Iris Dataset\n",
        "# Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "# Print the best parameters and the resulting model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Load Iris dataset explicitly for this task\n",
        "iris = load_iris()\n",
        "X_iris, y_iris = iris.data, iris.target\n",
        "\n",
        "# Train-test split for Iris dataset\n",
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
        "    X_iris, y_iris, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "param_grid = {\n",
        "    \"max_depth\": [2, 3, 4, 5],\n",
        "    \"min_samples_split\": [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    param_grid,\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "grid.fit(X_train_iris, y_train_iris)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRvZlgVVlF5M",
        "outputId": "e1bb8576-e0c3-4080-e3eb-67a5bf45f70e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Best Accuracy: 0.9416666666666668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "- Handling Missing Values:\n",
        "\n",
        "Numerical features can be filled using mean or median.\n",
        "\n",
        "Categorical features can be filled using the most frequent value.\n",
        "\n",
        "- Encoding Categorical Features:\n",
        "\n",
        "Use Label Encoding for ordinal data.\n",
        "\n",
        "Use One-Hot Encoding for nominal data.\n",
        "\n",
        "- Training the Decision Tree:\n",
        "\n",
        "Split data into training and testing sets.\n",
        "\n",
        "Train a Decision Tree Classifier using suitable impurity criteria.\n",
        "\n",
        "- Hyperparameter Tuning:\n",
        "\n",
        "Tune parameters like max_depth, min_samples_split using GridSearchCV.\n",
        "\n",
        "- Model Evaluation:\n",
        "\n",
        "Use accuracy, precision, recall, F1-score, and confusion matrix."
      ],
      "metadata": {
        "id": "KYeT3MPcmffI"
      }
    }
  ]
}